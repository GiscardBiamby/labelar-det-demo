{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add h4d proj root to sys.path so we can import stuff that's in h4d_main/h4d:\n",
    "# Example: suppose you want to import something from <h4d_main>/h4d/h4d/submodule/:\n",
    "# \"import h4d.submodule\" or \"from h4d.submodule import foo\"\n",
    "import sys, subprocess\n",
    "from pathlib import Path\n",
    "root_path = Path(subprocess.check_output(['git','rev-parse','--show-toplevel']).strip().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow._api.v1.version' from '/home/gbiamby/anaconda3/envs/labelar_demo/lib/python3.7/site-packages/tensorflow/_api/v1/version/__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "# tf.enable_eager_execution()\n",
    "tf.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gbiamby/school/labelar-det-demo\n"
     ]
    }
   ],
   "source": [
    "weights_path = root_path / \"training/weights/ssd_mobilenet_v2_bidmugs\"\n",
    "tfl_path = weights_path / \"tflite\"\n",
    "tfl_model_path = tfl_path / \"detect.tflite\"\n",
    "label_path = tfl_path / \"labelmap.txt\"\n",
    "print(root_path)\n",
    "assert weights_path.exists() and tfl_model_path.exists() and label_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_calibrate_quantize_model', '_grappler_config', '_has_valid_tensors', '_int8_target_required', '_is_calibration_quantize', '_is_post_training_optimize', '_is_weight_only_quantize', '_set_batch_size', '_tf_api_names', '_tf_api_names_v1', '_validate_representative_dataset', 'convert', 'from_frozen_graph', 'from_keras_model_file', 'from_saved_model', 'from_session', 'get_input_arrays']\n",
      "float16 post-training quantization enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4796584"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dir(tf.lite.TFLiteConverter))\n",
    "#    'from_frozen_graph','from_saved_model','from_session'\n",
    "converter = tf.lite.TFLiteConverter.from_frozen_graph(\n",
    "    str(tfl_path / \"tflite_graph.pb\")\n",
    "    , input_arrays = [\"image_tensor\"]\n",
    "     , output_arrays = ['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' ]\n",
    "    , input_shapes={\"image_tensor\":[1,300,300,3]}\n",
    ")\n",
    "converter.allow_custom_ops = True\n",
    "enable_quantize_fp16 = True\n",
    "if enable_quantize_fp16:\n",
    "    print(\"float16 post-training quantization enabled.\")\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "#     converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()\n",
    "open(tfl_path / \"converted_model.tflite\", \"wb\").write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mean, input_std = 127.5, 127.5\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "height = input_details[0]['shape'][1]\n",
    "width = input_details[0]['shape'][2]\n",
    "floating_model = input_details[0]['dtype'] == np.float32\n",
    "\n",
    "print(\"Model data type: \", input_details[0]['dtype'])\n",
    "print(\"input dims: h,w:\", height, width)\n",
    "\n",
    "# Input:\n",
    "# img_path = Path(\"/tmp/grace_hopper.bmp\")\n",
    "# assert img_path.exists()\n",
    "# img = Image.open(img_path).resize((width, height))\n",
    "# input_data = np.expand_dims(img, axis=0)\n",
    "# if floating_model:\n",
    "#     input_data = (np.float32(input_data) - input_mean) / input_std\n",
    "# Test the TensorFlow Lite model on random input data.\n",
    "# input_shape = input_details[0][\"shape\"]\n",
    "# input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "# interpreter.set_tensor(input_details[0][\"index\"], input_data)\n",
    "# interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor datLLa.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "# tflite_results = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "# Test the TensorFlow model on random input data.\n",
    "# tf_results = model(tf.constant(input_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labelar_demo",
   "language": "python",
   "name": "labelar_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
